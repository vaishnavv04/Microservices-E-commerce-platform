# Logstash Pipeline Configuration
# Receives JSON logs from microservices and sends to Elasticsearch

input {
  # TCP input for receiving logs from microservices
  tcp {
    port => 5044
    codec => json_lines
    type => "microservice-logs"
  }
  
  # Optional: UDP input for faster (but less reliable) log shipping
  udp {
    port => 5000
    codec => json_lines
    type => "microservice-logs"
  }
}

filter {
  # Parse timestamp if present
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
    }
  }
  
  # Add host information
  mutate {
    add_field => {
      "environment" => "${NODE_ENV:development}"
    }
  }
  
  # TODO: Add custom filtering rules here
  # Examples:
  # 
  # Filter out health check logs:
  # if [message] =~ /health check/ {
  #   drop { }
  # }
  #
  # Parse specific log patterns:
  # if [service] == "user-service" {
  #   grok {
  #     match => { "message" => "%{GREEDYDATA:custom_field}" }
  #   }
  # }
  #
  # Add geolocation from IP:
  # if [client_ip] {
  #   geoip {
  #     source => "client_ip"
  #   }
  # }
  
  # Ensure service name is present
  if ![service] {
    mutate {
      add_field => { "service" => "unknown" }
    }
  }
  
  # Add log level normalization
  if [level] {
    mutate {
      lowercase => ["level"]
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "ecommerce-logs-%{+YYYY.MM.dd}"
    # TODO: Enable authentication for production
    # user => "elastic"
    # password => "${ELASTIC_PASSWORD}"
  }
  
  # Debug: Also output to stdout for development
  stdout {
    codec => rubydebug
  }
}
